'''
Standalone script to improve portfolio repository based on today's consolidated job descriptions.
This script reads the consolidated CSV file generated by consolidate_today_jds.py and 
automatically improves the quant portfolio repository by adding missing functionality.
'''

import os
import sys
import csv
from datetime import datetime, date
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config.settings import quant_poc_repo_path, quant_poc_repo_remote, use_ai_for_requirements
from config.secrets import use_AI, ai_provider, github_token
from modules.git_integration import analyze_jd_requirements, identify_repo_gaps, generate_poc_improvements, commit_to_poc_repo
from modules.helpers import print_lg
from modules.ai.openaiConnections import ai_create_openai_client, ai_close_openai_client
from modules.ai.deepseekConnections import deepseek_create_client
from modules.ai.geminiConnections import gemini_create_client


def get_today_consolidated_csv(script_dir=None, date_override=None):
    '''
    Get the path to today's consolidated CSV file.
    
    Args:
        script_dir: Optional script directory path
        date_override: Optional date override (YYYYMMDD format)
        
    Returns:
        Path to consolidated CSV file or None if not found
    '''
    if script_dir is None:
        script_dir = Path(__file__).parent
    else:
        script_dir = Path(script_dir)
    
    # Determine date to use
    if date_override:
        target_date = date_override
    else:
        target_date = date.today().strftime('%Y%m%d')
    
    # Construct file path
    csv_file = script_dir / "all excels" / f"today_jobs_summary_{target_date}.csv"
    
    return csv_file if csv_file.exists() else None


def main(csv_file_path=None, date_override=None):
    '''
    Main function to improve portfolio based on today's consolidated JDs.
    
    Args:
        csv_file_path: Optional path to consolidated CSV file
        date_override: Optional date override (YYYYMMDD format)
    '''
    print_lg("=" * 80)
    print_lg("Portfolio Improvement from Today's Consolidated Job Descriptions")
    print_lg("=" * 80)
    
    # Check if portfolio repo path is set
    if not quant_poc_repo_path:
        print_lg("ERROR: quant_poc_repo_path is not set in config/settings.py")
        print_lg("Please set the path to your portfolio repository.")
        return
    
    if not os.path.exists(quant_poc_repo_path):
        print_lg(f"ERROR: Portfolio repository path does not exist: {quant_poc_repo_path}")
        print_lg("Please create the directory or update the path in config/settings.py")
        return
    
    # Get consolidated CSV file path
    if csv_file_path:
        consolidated_csv = Path(csv_file_path)
        if not consolidated_csv.exists():
            print_lg(f"ERROR: Consolidated CSV file not found: {csv_file_path}")
            return
    else:
        consolidated_csv = get_today_consolidated_csv(date_override=date_override)
        if not consolidated_csv:
            target_date = date_override if date_override else date.today().strftime('%Y%m%d')
            print_lg(f"ERROR: Consolidated CSV file not found for date: {target_date}")
            print_lg(f"Expected file: all excels/today_jobs_summary_{target_date}.csv")
            print_lg("Please run consolidate_today_jds.py first to generate the consolidated file.")
            return
    
    print_lg(f"\nUsing consolidated CSV: {consolidated_csv}")
    
    # Initialize AI client if needed
    ai_client = None
    if use_AI and use_ai_for_requirements:
        try:
            if ai_provider.lower() == "openai":
                ai_client = ai_create_openai_client()
            elif ai_provider.lower() == "deepseek":
                ai_client = deepseek_create_client()
            elif ai_provider.lower() == "gemini":
                ai_client = gemini_create_client()
            print_lg(f"Initialized {ai_provider} AI client")
        except Exception as e:
            print_lg(f"Failed to initialize AI client: {e}")
            print_lg("Continuing without AI (will use template-based generation)")
    
    # Read consolidated CSV and extract job descriptions
    print_lg(f"\nReading consolidated job descriptions from: {consolidated_csv}")
    all_requirements = {
        'skills': set(),
        'technologies': set(),
        'algorithms': set(),
        'tools': set(),
        'methodologies': set()
    }
    
    jobs_processed = 0
    job_titles = []
    
    try:
        with open(consolidated_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                job_desc = row.get('Job Description', '')
                job_title = row.get('Title', 'Unknown')
                
                if job_desc and job_desc != 'Unknown' and job_desc.strip():
                    jobs_processed += 1
                    job_titles.append(job_title)
                    print_lg(f"\nProcessing job {jobs_processed}: {job_title} at {row.get('Company', 'Unknown')}")
                    
                    # Analyze JD requirements
                    jd_req = analyze_jd_requirements(job_desc, ai_client)
                    
                    # Aggregate requirements
                    for key in all_requirements:
                        if isinstance(jd_req.get(key), list):
                            all_requirements[key].update(jd_req[key])
                        elif jd_req.get(key):
                            all_requirements[key].add(jd_req[key])
        
        if jobs_processed == 0:
            print_lg("\nNo valid job descriptions found in consolidated CSV.")
            print_lg("Please ensure the file contains job descriptions.")
            return
        
        print_lg(f"\nProcessed {jobs_processed} job description(s) from today's consolidation")
        print_lg("\nAggregated Requirements:")
        for key, values in all_requirements.items():
            if values:
                print_lg(f"  {key}: {', '.join(list(values)[:10])}")
                if len(values) > 10:
                    print_lg(f"    ... and {len(values) - 10} more")
    
    except FileNotFoundError:
        print_lg(f"ERROR: Consolidated CSV file not found: {consolidated_csv}")
        return
    except Exception as e:
        print_lg(f"Error reading consolidated CSV file: {e}")
        import traceback
        print_lg(traceback.format_exc())
        return
    
    # Convert sets to lists for gap identification
    aggregated_req = {k: list(v) for k, v in all_requirements.items()}
    
    # Check if we have any requirements
    has_requirements = any(aggregated_req.values())
    if not has_requirements:
        print_lg("\nNo requirements extracted from job descriptions.")
        print_lg("This may indicate the job descriptions don't contain quant-related keywords.")
        return
    
    # Identify gaps in portfolio
    print_lg(f"\nAnalyzing portfolio repository: {quant_poc_repo_path}")
    gaps = identify_repo_gaps(aggregated_req, quant_poc_repo_path)
    
    if not gaps:
        print_lg("\nNo gaps identified! Your portfolio already covers all requirements from today's jobs.")
        return
    
    print_lg(f"\nIdentified {len(gaps)} gap(s) in portfolio:")
    for i, gap in enumerate(gaps, 1):
        print_lg(f"  {i}. {gap}")
    
    # Generate improvements
    print_lg("\nGenerating portfolio improvements...")
    created_files = generate_poc_improvements(
        gaps, aggregated_req, ai_client, quant_poc_repo_path
    )
    
    if not created_files:
        print_lg("No new files were created (files may already exist).")
        return
    
    print_lg(f"\nCreated {len(created_files)} new file(s):")
    for file_path in created_files:
        print_lg(f"  - {os.path.relpath(file_path, quant_poc_repo_path)}")
    
    # Commit to repo
    print_lg("\nCommitting changes to repository...")
    success = commit_to_poc_repo(
        quant_poc_repo_path,
        created_files,
        gaps,
        quant_poc_repo_remote,
        github_token
    )
    
    if success:
        print_lg("\n[SUCCESS] Portfolio repository successfully improved!")
        print_lg(f"   Added {len(created_files)} new feature(s)")
        print_lg(f"   Repository: {quant_poc_repo_path}")
        print_lg(f"   Based on {jobs_processed} job description(s) from today")
    else:
        print_lg("\n[WARNING] Files created but commit may have failed. Check git status.")
    
    # Close AI client
    if ai_client and ai_provider.lower() == "openai":
        try:
            ai_close_openai_client(ai_client)
        except:
            pass
    
    print_lg("\n" + "=" * 80)
    print_lg("Portfolio improvement from today's JDs complete!")
    print_lg("=" * 80)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Improve portfolio repository based on today's consolidated job descriptions"
    )
    parser.add_argument(
        '--csv',
        type=str,
        help='Path to consolidated CSV file (default: auto-detect today\'s file)'
    )
    parser.add_argument(
        '--date',
        type=str,
        help='Date override in YYYYMMDD format (default: today)'
    )
    
    args = parser.parse_args()
    
    main(csv_file_path=args.csv, date_override=args.date)
